\documentclass[conference]{IEEEtran}

% Document properties
\title{On-chip Order-Exploiting Routing Table Minimization for a Multicast Supercomputer Network}
\author{%
  \IEEEauthorblockN{Andrew~Mundy, Jonathan~Heathcote and Jim~D.~Garside}
  \IEEEauthorblockA{School of Computer Science,
                    University of Manchester, UK}
}

% Prettier tables
\usepackage{booktabs}

% Pictures
\usepackage{graphicx}

% Diagrams
\usepackage{tikz}
\usepackage{ifthen}
\usetikzlibrary{ calc
               , backgrounds
               , positioning
               , decorations.pathreplacing
               , decorations.markings
               , arrows
               , positioning
               , automata
               , shadows
               , fit
               , shapes
               , arrows
               , patterns
               , spy
               }

% Pseudo-code
\usepackage{clrscode3e}

% Force float positions in some cases
\usepackage{float}

% Better math environments
\usepackage{amsmath}

% Neaten the fixed-width font
\usepackage{sourcecodepro}
\newcommand{\mytt}[1]{\texttt{\footnotesize#1}}

\usepackage[binary-units]{siunitx}
\usepackage[caption=false, font=footnotesize]{subfig}

% Crossing things out
\usepackage{cancel}

%% Bibliographies
\usepackage[backend=bibtex, style=ieee, doi=false, url=false, mincitenames=1, maxcitenames=2, maxbibnames=7]{biblatex}
\renewcommand*{\bibfont}{\small}
\renewbibmacro{finentry}{%
  \iffieldequalstr{entrykey}{Navaridas2015}%<- key after which you want the break
  {\finentry\newpage}
{\finentry}}
\bibliography{paper}

\begin{document}
  \maketitle

  \begin{abstract}
SpiNNaker is a many-core supercomputer -- designed for the simulation of large neural-networks -- in which cores communicate with \emph{multicast} packets.
Routing within SpiNNaker is controlled by Ternary Content Addressable Memories (TCAMs) of quite limited size.
As not all neural-network applications will result in routing tables sufficiently small to fit in TCAM some minimization is necessary.
In this paper we argue that existing techniques neither result in sufficiently minimized tables nor can be implemented within the small code and memory footprint available to a SpiNNaker core.
To resolve these issues we present a new algorithm, Ordered-Covering (OC), which exploits the ordered nature of TCAMs to achieve good compression of routing tables while meeting the code-space and memory constraints of the SpiNNaker platform.
We show that, for one benchmark, on-chip routing table minimization using OC results in a 64.5$\times$ speed-up compared with performing the minimization off-chip.
For a second, more challenging, benchmark we show that a 2.8$\times$ speed-up in table minimization time is achieved by combined on- and off-chip minimization.
  \end{abstract}

  \section{Introduction}

SpiNNaker~\parencite{Furber2014} is a purpose-built supercomputer intended to facilitate the simulation of large biological neural networks.
Software models are distributed across a network of processors which communicate by passing (short) messages.
Neurons in the brain typically have a `fanout' of the order of thousands~\parencite{Kung1988} and the structure of the interconnection network reflects this in that it supports multicasting packets to an arbitrary set of destinations.

Each chip in the system has a single router capable of steering packets to or from the 18~on-chip processor cores and inter-chip links.
The inter-chip network is a triangularly connected 2D surface, which may be wrapped into a torus to provide greater aggregate bandwidth.
This can be drawn as a hexagonal mesh but is sometimes more conveniently pictured skewed into squares with the six inter-chip links labelled with compass points (\figurename~\ref{fig:diagrams/topology}).

\begin{figure}[!b]
  
  \center
  \input{diagrams/topology}
  
  \caption{SpiNNaker chips (squares) in a triangular torus topology. Wrap-around links omitted for clarity.}
  \label{fig:diagrams/topology}
  
\end{figure}

Packets are routed around this network by Address Event Representation (AER)~\parencite{Boahen2000}; each packet contains a \emph{routing key} which, typically, identifies its source and routers forward (and may duplicate) packets towards their destination(s).
32-bit routing keys are used, which reflect the intended scale of the system -- $10^9$ neurons, simulated across $10^6$ processing cores.
At each router packets are recognised using a Ternary Content Addressable Memory (TCAM) which bit-masks keys before looking for particular matches,
this means that each key bit can be matched with binary \mytt{0}, \mytt{1}, or \mytt{X} (`don't care').
The TCAM is prioritised such that only the first-found match will be returned, allowing `catch all' entries to be inserted near the bottom of the table.
If a key is not found in the table, the associated packet is `default-routed': continuing in the direction it arrived.

For example, given a simplified routing table with 4-bit keys

\begin{table}[H]
  \centering
  \begin{tabular}{c l}
    \toprule
    Key-Mask & Route \\
    \midrule
    \texttt{0000} & \texttt{NE N}\\
    \texttt{X111} & \texttt{S}\\
    \texttt{1XXX} & \texttt{3 4}\\
    \bottomrule
  \end{tabular}
\end{table}

\noindent packets with the key \mytt{0000} would match only the first entry and would be routed out of both the North East and North links.
Any packets with the keys \mytt{0111} or \mytt{1111} would match the second entry in the table and be routed out of the South link.
Other packets beginning with a \mytt{1} would match the last entry and be routed to the indicated processing cores on the same chip as the router.
All other packets would be unrecognised and be default-routed in a straight line, e.g., a packet arriving on the South West link with the key \mytt{0011} would be routed out of the North East link.

The TCAM was chosen with an arbitrary size of \num{1024}~entries, believed large enough for most applications although making entries quite precious.
Indeed, it is not always possible to avoid exceeding this limit on routing table size -- particularly for neural networks which feature large `fanin' or `fanout' routes.
Exploiting the massive parallelism of SpiNNaker to perform any minimization of routing tables is desirable due to the large number of tables to minimize, \num{57600} in the largest configurations of machine.
As we will show, this presents a challenge due to the limited on-chip memory available.

\section{Routing table minimization}

\subsection{Benchmarks}

To indicate the importance of routing table minimization to SpiNNaker we constructed two benchmark networks.
In the first of these, each processing core in a $12\times12$, 144-chip (\num{2592}-core) system is connected to every other core in the system with a probability determined by the distance between the cores.
This benchmark models networks in which processing cores are strongly connected to their neighbours but are weakly connected to distant chips:
similar forms of distance-dependent connectivity are found in some brain regions (e.g., \parencite{Hellwig2000}).

The second benchmark extends the first benchmark by adding a number of longer distance connections to the network and is based on the `centroid' model of \textcite{Navaridas2015}.
In this model a small number of cores transmit packets to another group of cores located at some distance across the machine.
% Each core has a \SI{10}{\percent} chance of being connected to a single, distant, group of cores and a \SI{5}{\percent} chance of being connected to two distant groups.
\figurename~\ref{fig:experiment/setup} illustrates these two benchmarks and shows the probability of a single core being connected to other cores within the network.

\begin{figure}
  \centering
  \includegraphics{experiments/experiments.pdf}
  \caption{Map of the probability of a single core being connected to cores on surrounding chips for the two benchmarks. Darker means higher probability.
           In the above centroid model the core is connected to two clusters of cores (toward the South West and North West).}
  \label{fig:experiment/setup}
\end{figure}

Routing keys were assigned such that packets originating on the $p$th core of the chip at $(x, y)$ in the skewed mesh used the two most significant bytes in the 32-bit key to represent the $x$ and $y$ co-ordinates and the following five bits to indicate the core ID, $p$.
The remaining 11 bits of the key were left blank, i.e.,~\mytt{X} in the routing tables.
This `XYP' key assignation scheme is common in SpiNNaker applications~\parencite{Davies2012}.

Once the connectivity of the benchmarks was determined, the Neighbour Exploring Routing (NER) algorithm~\parencite{Navaridas2015} was used to generate the routes taken by packets across the network.
While this algorithm has been shown to exploit `default routing' to reduce the size of routing tables, the routing tables of both our benchmarks were too large to fit in the number of entries available, even when entries which could be replaced by default routing were removed (see \figurename~\ref{fig:results/final}, at the end of the paper).

\subsection{Routing table compaction}

\textcite{Liu2002} presented a method for using Espresso-II~\parencite{Brayton1984} to minimize routing tables for IP routers.
In this method a routing table is partitioned in subtables in which entries have equivalent routing destinations and prefix length.
Each subtable is minimized using Espresso to produce a functionally equivalent but smaller set of entries.
\figurename~\ref{fig:results/final} shows the result of applying this technique to the routing tables from our benchmarks once entries which could be replaced by default routing were removed.
In the case of the `locally-connected' benchmark a combination of default routing and logic minimization is able to reduce the majority, but not all, of the 144 routing tables to fewer than \num{1024}~entries.
In the more challenging centroid model this technique is unable to reduce \emph{any} tables to fit within the TCAM constraint.

In contrast to the unicast IP routing tables, at which minimization is normally targeted, SpiNNaker routing tables are multicast.
Consequently, there are two factors which may explain the poor compression ratios achieved by the above technique when applied to SpiNNaker routing tables.
Firstly, while the IP routing tables discussed by \textcite{Liu2002} have relatively few output routes (e.g., tens), each SpiNNaker router has the equivalent of up to $2^{24}$ unique routes (any combination of six inter-chip links and 18 processors) -- consequently the number of entries with equivalent routes can be expected to be much smaller.
Secondly, whereas IP addresses are assigned such that very coarse routing decisions may be made from very few bits, multicasting potentially reduces the amount of mutual information between keys which describe similar routes since keys are typically related to route origins, not destinations.

\subsection{``Order-exploiting'' minimization}

Unlike IP routing tables, SpiNNaker routing tables can be expected to be largely static.
As such, logic minimization which does not preserve exact equivalence, but instead matches a superset of the routing keys in use, may be used to generate routing tables with far fewer entries.
For example, \mytt{0001} and \mytt{0010} may be combined into a new entry (\mytt{00XX}) which matches a superset of the two keys originally matched.
In IP routing tables, which are considerably larger than those in SpiNNaker and are highly dynamic, this minimization would be avoided as it would (a) require the minimizer to inspect the entire table during minimization to avoid breaking the functionality of the table,
and (b) slow down the process of updating the table to deal with changed routes.
However, in the case of SpiNNaker -- and other systems with largely static routing tables and very tight constraints on entries -- this technique can allow for significant reductions in table size.

We extended the technique of \textcite{Liu2002} to investigate the effect of this on our benchmark networks.
First, each routing table was split into subtables of entries with the same outgoing route.
Next the subtables were sorted such that subtables with fewer entries were moved to the top of the routing table and larger ones toward the bottom.
Finally, the minimization procedure from Espresso was applied to each subtable in turn.
However, unlike the technique proposed by \textcite{Liu2002}, we allowed Espresso to generate entries which matched a superset of keys matched by the original entry provided that they did not match keys expected to match entries in lower subtables.
The minimized table depends on TCAM ordering to ensure that it is functionally equivalent to the original.
We call this use of table ordering to achieve greater compression of entries ``order-exploiting'' minimization.
\figurename~\ref{fig:results/final} shows that this new technique for reducing routing table size is sufficient to minimize all the routing tables in our benchmarks to fit within the 1024-entry size required by SpiNNaker.

Default routing cannot generally be used with order-exploiting minimization, as order-exploiting minimization could generate entries which match keys which would otherwise be default routed.
To avoid this problem we only minimize tables which contain entries representing every key that is expected to arrive at the router, including those which would ordinarily be default-routed.
As shown by \figurename~\ref{fig:results/final}, tables constructed using order-exploiting minimization (which do not rely on default routing) are smaller, for our benchmarks, than the original tables with default-routes removed and then minimized with non-order-exploiting minimization.

\subsection{On-chip logic minimization}

Espresso is capable of minimizing routing tables of the scale shown in our benchmarks; a mean-time of \SI{6.23}{\second} was spent in Espresso when minimizing a table from the locally-connected benchmark on a lightly-loaded Intel~Pentium~G850.
However, as projected SpiNNaker machines are expected to consist of nearly sixty-thousand chips, and the same number of routing tables, the  compute time required to minimize all tables is likely to be significant -- more than four days if the locally-connected benchmark were scaled to the size of a full SpiNNaker machine.
Fortunately, minimization on this scale is an `embarrassingly'-parallel problem, and the wall-clock time required to perform this task for SpiNNaker may be significantly reduced by performing the minimization directly on the chips to which the routing tables relate.

Processing cores in SpiNNaker have access to \SI{64}{\kibi\byte} of Data Tightly-Coupled Memory (DTCM) and \SI{32}{\kibi\byte} of Instruction Tightly-Coupled Memory (ITCM).
This places severe limitations on the programs which they may execute.
While we have shown that Espresso may be used to perform order-exploiting minimization for use with SpiNNaker, its memory usage and code size preclude its use on embedded systems~\parencite{Lysecky2003}.
In contrast, m-Trie minimization~\parencite{Ahmad2007} has been shown to be capable of effectively compressing IP routing tables while consuming very little memory (as low as \SI{16}{\kibi\byte}) but is not suitable for use when order-exploiting minimization is required (see \figurename~\ref{fig:results/final}) as it will not generate entries which match a superset of the original keys.

\section{Ordered-Covering}

  We present a novel algorithm called ``Ordered-Covering'' which is capable of performing order-exploiting routing table minimization within the small memory and code-space available on SpiNNaker.
  The algorithm proceeds by sequentially \textit{merging} sets of routing table entries with the same output routes while ensuring these merged entries do not \textit{cover} existing entries. 
  A set of routing entries is merged by replacing them with one new entry containing only the bits common to all the original entries and \mytt{X}s for all other bits
  (e.g., \mytt{1010} and \mytt{0110} would be merged to produce \mytt{XX10}).
  A routing entry covers another when the set of keys matched by one entry intersects with those of another below it in the table.

  In addition, we annotate each entry in the table with the set of keys that it is expected to match, the \textit{aliases} of the entry.
  For example, merging the first two entries in the following table would generate a new entry with the original keys listed as aliases:

  \begin{figure}[H]
    \centering
    \includegraphics{figures/aliases_example}
  \end{figure}

  \begin{figure*}
    \subfloat[%
      A merge which does not obey the \textit{up-check} rule.
      Before the merge a packet with key \mytt{0011} would have been routed to \mytt{E} \mytt{S}; after the merge the same packet would be routed to \mytt{N} instead.
      This is because the merge has resulted in the correct entry being moved below an entry which \textit{covers}~it.
    ]{%
      \label{fig:algorithm/rule2a_example}
      \begin{minipage}{.95\columnwidth}
        \centering
        \includegraphics{figures/rule2a_example}
      \end{minipage}
    }
    \hfill
    \subfloat[%
      A merge which does not obey the \textit{down-check} rule.
      Before the merge a packet with key \mytt{1100} would have been routed to \mytt{NE} \mytt{S}; after the merge the same packet would be routed to \mytt{SW} \mytt{2} instead.
      This is because the merge has resulted in the insertion of a new entry which \textit{covers} an existing~entry.
    ]{%
      \label{fig:algorithm/rule2b_example}
      \begin{minipage}{.95\columnwidth}
        \centering
        \includegraphics{figures/rule2b_example}
      \end{minipage}
    }
    \caption{Examples of invalid merges as defined by the \textit{Ordered-Covering} rules}
    \label{fig:algorithm/rules}
  \end{figure*}

  When merging routing table entries, the following \textit{Ordered-Covering rules} are used to ensure the behavior of the table remains consistent:

  \begin{description}[\IEEEsetlabelwidth{\emph{Down-check} rule:}]
    \item[\emph{Up-check} rule:]
      No entry in the merge may become \textit{covered} by an entry higher up the table.
      A merge that would be disallowed by this rule is shown in \figurename~\ref{fig:algorithm/rules}\subref{fig:algorithm/rule2a_example}.
    \item[\emph{Down-check} rule:]
      No \textit{aliased entry} below the merge may become \textit{covered},
      see \figurename~\ref{fig:algorithm/rules}\subref{fig:algorithm/rule2b_example}.
  \end{description}

    In addition, as a heuristic, we keep the table sorted in increasing order of \textit{generality}, i.e., the number \mytt{X}s contained in their keys and masks.
    For example, an entry with the key-mask of \mytt{00XX} (\textit{generality} of 2) must be placed below any entries with fewer \mytt{X}s in their key-masks, e.g., below \mytt{0000} and \mytt{00X1} (generalities of 0 and 1 respectively).
    This is only a partial ordering of the table, and as an additional heuristic we require that new entries of generality $g$ be inserted \emph{above} existing entries of generality $g$.
    For example, if \mytt{XX00} were already present in the table the new entry \mytt{0XX1} must be inserted above it.

  Ordered Covering uses a simple, greedy, algorithm to minimize a routing table according to these rules:\par\nopagebreak
  \begin{codebox}
    \Procname{$\proc{Minimize}(\id{table}, \id{target})$}
    \li \While $\attrib{table}{length} >  \id{target}$
    \li \Do $\id{merge} \gets \proc{Get-Largest-Merge}(\id{table})$
    \li     \If $\attrib{merge}{isEmpty}$
    \li     \Then \kw{break} \End
    \li     $\id{table} \gets \proc{Apply-Merge}(\id{table}, \id{merge})$
        \End
    \li \Return $\id{table}$
  \end{codebox}

  \noindent Where:
  \begin{itemize}
    \item $\id{table}$ is a routing table that is sorted according to rule~(1) and contains entries for all packets expected to arrive at the router, \textit{including those which could be handled by default routing}
    \item $\proc{Get-Largest-Merge}$ is a function which returns a (possibly empty) set of routing table entries which may be merged without breaking the up- or down-check rules~(2)
    \item $\proc{Apply-Merge}$ merges the set of routing entries and uses the table-ordering heuristics to determine where the new entry should be inserted into the table
  \end{itemize}

  The algorithm terminates early if $\attrib{table}{length} \le \id{target}$ which can save a large amount of time in comparison to fully minimizing the table.
  Commonly, $\id{target}$ will be set to the number of entries available in the table (e.g., 1024).

  The $\proc{Get-Largest-Merge}$ function is where the algorithm spends most of its time and we present one possible implementation in the remainder of this section.
  Our implementation considers merging all groups of entries in the table which share the same set of output ports.
  In practice, merging these groups will often violate either the up-check or down-check rules.
  Rather than immediately rejecting these merge candidates, we use a pair of simple greedy approaches to iteratively remove entries from merge candidates until both rules are obeyed.
  We split our approach into two phases, the first refines merge candidates until they obey the up-check rule and the second does the same for the down-check rule.

  \subsection{Resolving the up-check}

  The \textit{up-check} rule ensures that an entry, $e$, cannot -- through being merged with other entries -- become covered by being moved below another entry which matches a subset of the keys matched by $e$.
  
  For each merge candidate we inspect every entry in the merge to ensure it does not break the up-check rule.
  When inspecting an entry, $e$, in the merge candidate we scan through the entries between $e$ and the position where the merged entry would be inserted.
  If any entry is found which would cover $e$, $e$ is removed from the merge candidate and we proceed to check the next entry in the candidate merge.

  For example, consider the table:\par\nopagebreak
  \begin{figure}[H]
    \centering
    \includegraphics{figures/upcheck_resolve_example_1}
  \end{figure}

  \noindent Initially we could consider merging the first three entries.
  Doing so would mean that \mytt{0000} would be combined into the new entry \mytt{XXX0}, positioned below \mytt{00XX}, causing \mytt{0000} to become covered.
  As the entry matching \mytt{0000} is moved below \mytt{00XX} by including it in the merge, \mytt{0000} must be removed from the merge to avoid covering it.
  After removing \mytt{0000} the, refined, merge is:\par\nopagebreak
  \begin{figure}[H]
    \centering
    \includegraphics{figures/upcheck_resolve_example_2}
  \end{figure}

  \noindent Since the remaining entries in the merge candidate (\mytt{1000} and \mytt{1110}) are not covered by \mytt{00XX} (the only entry between their current positions and the location where the merge is inserted), the refined merge obeys the up-check rule.

  \subsection{Resolving the down-check}

  The \textit{down-check} rule ensures that the entry resulting from a merge would not match a subset of the packets expected to match entries below the point where the entry resulting from the merge would be located.
  
  For example, consider the table:\par\nopagebreak
  \begin{figure}[H]
    \centering
    \includegraphics{figures/downcheck_resolve_example_1}
  \end{figure}

  \noindent Our initial merge candidate, the first three entries, would result in the insertion of the entry \mytt{0XXX} immediately above the bottom entry.
  However, \mytt{0XXX} matches \mytt{0101}: one of the aliases of \mytt{XXXX}.
  Consequently, merging the first three entries as indicated is not allowed as it violates the down-check rule, changing the behavior of the routing table.

  Once we have identified that the entry resulting from a merge, $e_m$ (e.g., \mytt{0XXX}), covers an entry lower in the table, $e_c$ (e.g., \mytt{0101}), we attempt to modify the merge to avoid the covering.
  This may be achieved by converting one of the \mytt{X}s in $e_m$ so that it is the opposite of the bit in the same position in $e_c$.
  In our example we look to remove entries from the merge such that one of the \mytt{X}s in \mytt{0XXX} such that the new entry resulting from the merge, $e_m^\prime$, is \mytt{0\underline{0}XX}, \mytt{0X\underline{1}X} or \mytt{0XX\underline{0}} -- none of which would cover $e_c$ (\mytt{0101}).
  
  An \mytt{X} in the entry resulting from a merge may be converted to another value, $v$, by removing from the merge any entries which have either \mytt{X} or $\overline{v}$ in the given bit position.
  In our example, to convert \mytt{0XX\underline{X}} to \mytt{0XX\underline{0}} we remove from the merge any entries with \mytt{X} or \mytt{1} in the rightmost bit position, in this instance we would remove \mytt{001\underline{1}} and \mytt{011\underline{X}} from the merge.
  Alternatively, to convert \mytt{0X\underline{X}X} to \mytt{0X\underline{1}X} we remove from the merge any entries with \mytt{X} or \mytt{0} in the indicated position, here we would need to remove only \mytt{00\underline{0}0}.
  Finally, to convert \mytt{0\underline{X}XX} to \mytt{0\underline{0}XX} we would need to remove \mytt{0\underline{1}1X} from the merge.
  As a result of removing entries we have three new merge candidates:\nopagebreak
  
  \begin{align*}
    \{\mytt{0000},~\cancel{\mytt{0011}},~\cancel{\mytt{011X}}\} &~\rightarrow~ \mytt{000\underline{0}} \\
    \{\cancel{\mytt{0000}},~\mytt{0011},~\mytt{011X}\}          &~\rightarrow~ \mytt{0X\underline{1}X} \\
    \{\mytt{0000},~\mytt{0011},~\cancel{\mytt{011X}}\}          &~\rightarrow~ \mytt{0\underline{0}XX} \\
  \end{align*}
  
  We immediately reject the merge candidate with the fewest entries, leaving us with two candidates containing two entries each.
  As a heuristic, we select the merge candidate which converted the most significant \mytt{X} bit, in this case resulting in the merge:\par\nopagebreak
  \begin{figure}[H]
    \centering
    \includegraphics{figures/downcheck_resolve_example_2}
  \end{figure}
  
  We repeat the above process until either no entries are covered (that is $e_m$ does not cover any $e_c$) and a valid merge is produced; or it is not possible to modify the merge to avoid covering a lower entry and the merge must be abandoned.
  In our example, the new merge candidate results in an entry \mytt{00XX} being inserted between the two remaining entries.
  As this new entry does not cover any other entries in the table the algorithm terminates.
  
  In our example there were three bits in $e_m$, \mytt{0\underline{XXX}}, which could be converted to avoid covering $e_c$.
  Precisely which bits may be converted depends upon the nature of the covered entry, for example, if \mytt{0XXX} were covering \mytt{010X} then only two bits (\mytt{0\underline{XX}X}) may be converted.
  We prioritize the uncovering of entries for which fewer bits may be converted as a heuristic intended to reduce the number of entries likely to be removed from the merge in the merge candidates considered.

  \section{Results}

  \subsection{Compression}

\begin{figure*}
  \centering
  \includegraphics{experiments/results_esp_and_oc}
  \caption{Compression performance of all the routing table minimization schemes for both benchmarks.
           Each violin shows the histogram of routing table sizes.
           Ordered-Covering minimizes all of the tables in the locally-connected benchmark, and
           \SI{66}{\percent} of the tables in the centroid benchmark, to $\le 1024$ entries.
           }
  \label{fig:results/final}
\end{figure*}

  \figurename~\ref{fig:results/final} shows the performance of Ordered-Covering, running on SpiNNaker, when used to minimize the benchmark routing tables.
  OC reduces all of the tables in the locally-connected benchmark to fit in the router TCAM.
  While this is not true of the centroid benchmark, OC is able to reduce \SI{66}{\percent} of the tables to fewer than 1024 entries.
  This more than halves the number of tables which would otherwise need to be minimized off-chip.


  \subsection{Memory usage}

% Gaussian: 18376 bytes of which 13668 bytes is the table
% Centroid: 19152 bytes of which 14328 bytes is the table

  On-chip routing table minimization is required to work with very small amounts of working memory.
  In order-exploiting minimization this small amount of memory must be used to store the whole routing table -- e.g., \SI{24}{\kibi\byte} for a 2048-entry table -- and any additional structures.
  Consequently, those additional structures must be memory efficient.
  After the routing table, the aliases table (which annotates routing table entries with the keys they are expected to match) is the most expensive structure in Ordered-Covering.
  Other structures, such as sets of entries being considered as merge candidates can be implemented using bit-vectors and consume very little memory.

  Each merge of $n$ entries performed by Ordered-Covering reduces the size of the routing table by $n-1$ entries (reducing the memory required to represent the table by $3(n-1)$~words) and adds a new entry into the aliases table.
  In our implementation of the aliases table, which uses AA-trees and linked-lists of fixed-size blocks, a merge of $n$ entries will increase the memory required by the aliases table by $2(n + 1) + 4$~words.
  Consequently, the total memory cost of a merge of $n$ entries is $2(n + 1) + 4 - 3(n - 1)$~words.
  If the space saved by minimizing the routing table were reclaimed then any merge of ten or more entries would reduce the memory required to represent the routing and the aliases tables.

  Our implementation of Ordered-Covering does not reclaim the memory saved by reducing the size of a routing table, meaning that the routing table remains a constant size while the aliases table grows with every merge.
  The peak heap usage of our minimizer when running on SpiNNaker was \SI{18.4}{\kibi\byte} for the locally-connected benchmark (of which \SI{13.3}{\kibi\byte} was used to represent the routing table) and \SI{18.8}{\kibi\byte} for the centroid model (\SI{14.0}{\kibi\byte} for the routing table).
  Ordered-Covering is thus able to achieve good levels of table compression for very little memory cost above that required to represent the routing table.

  \subsection{Execution time}

  \begin{table}
    \centering
    \caption{Time to load and minimize 144 benchmark tables using Ordered-Covering on Spinnaker (NB: Not all tables were sufficiently minimised in the centroid model)}
    \label{table:results/oc-timing}
    \begin{tabular}{l S S S}
      \toprule
      & & \multicolumn{2}{c}{Exec. time / \si{\second}} \\
      Model & {Load time / \si{\second}} & {Sufficient} & {Fully} \\
      \midrule
      Locally-connected & 3.8 & 13.9 & 25.6 \\
      Centroid & 3.6 & & 25.6 \\
      \bottomrule
    \end{tabular}
  \end{table}

  \begin{table}
    \centering
    \caption{Time to minimize 144 benchmark tables using order-exploiting~Espresso on an Intel~Pentium~G850}
    \label{table:results/oe-espresso-timing}
    \begin{tabular}{l S S}
      \toprule
            &                                  & {Mean time} \\
      Model & {Cumulative time / \si{\second}} & {per table / \si{\second}} \\
      \midrule
      Locally-connected & 897.2 & 6.2 \\
      Centroid & 1146.8 & 8.0 \\
      \bottomrule
    \end{tabular}
  \end{table}

  Table~\ref{table:results/oc-timing} indicates the time required to load and minimize both of the benchmark networks using Ordered-Covering on SpiNNaker.
  Routing tables were minimized in parallel across the machine; while increasing the size of the benchmark would increase the time taken to load the tables to the machine it would not increase the time taken to minimize the tables.
  Table~\ref{table:results/oe-espresso-timing} shows the time taken to minimize the same benchmarks serially on an Intel~Pentium~G850 using order-exploiting Espresso.

  As Ordered-Covering was able to minimize all tables in the locally-connected benchmark to fit within TCAM no tables would need to be minimized off-chip.
  Consequently, on-chip OC represents a 64.5$\times$ speed-up when compared to using order-exploiting Espresso.

  Ordered-Covering was only able to minimize \SI{66}{\percent} of routing tables in the centroid benchmark to fit in TCAM.
  The remainder of these tables could then be minimized using order-exploiting Espresso, and the total time taken to minimize tables using this joint approach would be around \SI{413}{\second}.
  In this instance combined use of OC and Espresso represents a 2.8$\times$ speed-up when compared to just using order-exploiting Espresso.

  As routing tables may be minimized in parallel the time taken to minimize tables using order-exploiting Espresso could be reduced by using more processing cores.
  However, significantly reducing the time taken to minimize routing tables of the largest SpiNNaker machine (\num{57600} tables) will prove challenging without making use of the machine itself.

% \subsection{Hedging}
    
% Each SpiNNaker chip has 18 processor cores and one routing table enabling parallel table minimisation approaches.
% It is possible to execute several table minimisation algorithms simultaneously, terminating as soon as any algorithm produces a table which fits.
% This approach may be valuable in cases where default routing is sufficient since these routes are very cheap to compute.

  \section{Conclusion}

  In this paper we have shown that routing table minimization for SpiNNaker, a massively parallel supercomputer with a multicast interconnect network, is a challenging problem.
  In particular we indicated that neither the use of `default routing' nor existing minimization techniques were able to reduce tables from two benchmark models to fit within the limited number of TCAM entries in a SpiNNaker router.
  We extended the minimization technique of \textcite{Liu2002} to use Espresso to generate functionally equivalent, compressed, routing tables which matched a superset of the packets matched by the original tables and fit within the available number of entries.
  However, due to the limited code-space and memory available on the machine, this technique was not suitable for direct implementation on-chip: a desirable feature as routing table minimization is `embarassingly'-parallel and the largest SpiNNaker system will contain nearly sixty-thousand routing tables.

  To overcome these issues we presented Ordered-Covering, a novel algorithm for routing table minimization capable of running within the small amount of memory available to a SpiNNaker processing core.
  Using Ordered-Covering, in parallel on SpiNNaker, we were able to minimize all the tables in one of our benchmarks in \SI{17.7}{\second} -- a speed-up of 64.5$\times$ compared to off-chip minimization using Espresso.
  In the other benchmark, combined use of on- and off-chip minimization using Ordered-Covering and Espresso was able to achieve a speed-up of 2.8$\times$ of the time taken by off-chip minimization alone.

\section*{Notes}

\noindent An extensively annotated and `production-ready' Python implementation of the Ordered-Covering algorithm is available as part of the `Rig' SpiNNaker library: \url{https://goo.gl/uhTCpa}

\section*{Acknowledgements}

\noindent The authors would like to thank James Knight, Simon Davidson and Luis A.\ Plana for their helpful feedback on drafts of this paper.
The SpiNNaker project has been supported by EPSRC grants EP/D07908X/1 and EP/G015740/1~(BIMPA).

\printbibliography
\end{document}
